{
 "metadata": {
  "name": "",
  "signature": "sha256:f8367c3e02beb77b87107fbeb0c71025eb92a6f5b2b51f5ed062177d28715b98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lecture 11 - diagonalising matrices\n",
      "\n",
      "By an appropriate transformation, most matrices can be made diagonal. The diagonal form of a matrix can be convenient for a number of purposes. For one, it is easy to interpret the action of a diagonal matrix on a vector - it simply rescales the $i$th vector component by the $(i, i)$ component of the matrix. Some operations, such as raising a matrix to a power or computing the determinant, are trivial for diagonal matrices.\n",
      "\n",
      "## Diagonalisation\n",
      "\n",
      "Most matrices can be diagonalised. It is shown in the lecture notes that\n",
      "\n",
      "$$\n",
      "\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{-1}\n",
      "$$\n",
      "\n",
      "where the $i$th column of $\\boldsymbol{U}$ is the $i$th eigenvector of $\\boldsymbol{U}$, and $\\boldsymbol{\\Lambda}$ is a diagonal matrix where $\\Lambda_{ii}$ is the $i$th eigenvalue.\n",
      "\n",
      "If $\\boldsymbol{A}$ is invertible (no zero eigenvalues), then $\\boldsymbol{U}^{-1}$ exists. Therefore\n",
      "\n",
      "$$ \n",
      "\\boldsymbol{\\Lambda} = \\boldsymbol{U}^{-1} \\boldsymbol{A} \\boldsymbol{U}\n",
      "$$\n",
      "\n",
      "We can explore this property with a concrete examples. We first create a matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import NumPy and seed random number generator to make generated matrices deterministic\n",
      "import numpy as np\n",
      "np.random.seed(2)\n",
      "\n",
      "# Create a matrix with random entries\n",
      "A = np.random.rand(4, 4)\n",
      "print(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.4359949   0.02592623  0.54966248  0.43532239]\n",
        " [ 0.4203678   0.33033482  0.20464863  0.61927097]\n",
        " [ 0.29965467  0.26682728  0.62113383  0.52914209]\n",
        " [ 0.13457995  0.51357812  0.18443987  0.78533515]]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can compute the eigenvectors and eigenvalues using the NumPy function `linalg.eig`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute eigenvectors of A\n",
      "evalues, evectors = np.linalg.eig(A)\n",
      "\n",
      "print(\"Eigenvalues: {}\".format(evalues))\n",
      "print(\"Eigenvectors: {}\".format(evectors))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Eigenvalues: [ 1.60005805+0.j          0.07591753+0.13825201j  0.07591753-0.13825201j\n",
        "  0.42090561+0.j        ]\n",
        "Eigenvectors: [[ 0.45781186+0.j         -0.33035733-0.48998366j -0.33035733+0.48998366j\n",
        "  -0.66963039+0.j        ]\n",
        " [ 0.48624863+0.j         -0.61697886+0.j         -0.61697886-0.j\n",
        "  -0.08306861+0.j        ]\n",
        " [ 0.54605694+0.j         -0.01299484+0.11318301j -0.01299484-0.11318301j\n",
        "  -0.44437871+0.j        ]\n",
        " [ 0.50575922+0.j          0.48202011+0.15746263j  0.48202011-0.15746263j\n",
        "   0.58925572+0.j        ]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matrix `A` is non-symmetric, hence it is no surprise that the eigenvalues and eigenvectors are complex. The $i$th column of `evectors` (`A[:,i]`) is the $i$th eigenvector. \n",
      "\n",
      "We can now verify that $\\boldsymbol{\\Lambda} = \\boldsymbol{U}^{-1} \\boldsymbol{A} \\boldsymbol{U}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Lambda = np.linalg.inv(evectors).dot(A.dot(evectors))\n",
      "print(Lambda)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.60005805e+00 -4.12271839e-17j  -1.33117023e-16 +2.05865297e-16j\n",
        "   -1.37499840e-16 -1.96590549e-16j   5.55111512e-17 -1.47186519e-18j]\n",
        " [  2.77555756e-16 +1.11022302e-16j   7.59175252e-02 +1.38252015e-01j\n",
        "    5.55111512e-17 -1.11022302e-16j   3.46944695e-16 +6.93889390e-17j]\n",
        " [  2.77555756e-16 -4.44089210e-16j   3.46944695e-17 +1.11022302e-16j\n",
        "    7.59175252e-02 -1.38252015e-01j   3.60822483e-16 -5.55111512e-17j]\n",
        " [ -5.55111512e-16 -1.22305447e-17j   1.47058685e-17 -8.74158316e-18j\n",
        "   -8.28080646e-19 +1.90139925e-17j   4.20905607e-01 -8.21244250e-18j]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the matrix `Lambda` ($\\boldsymbol{\\Lambda}$) is diagonal, and the diagonal entries are the eigenvalues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Diagonalisation of symmetric matrices\n",
      "\n",
      "In the above, we have only required that the eigenvectors be linearly independent. In this case, the matrix $\\boldsymbol{U}$ can be inverted. For a symmetric matrix, we have proved in the lecture notes that the eigenvectors are orthogonal. Therefore, for a real, symmetric matrix $\\boldsymbol{S}$, if the eigenvectors have been normalised the matrix $\\boldsymbol{U}$ is an orthogonal matrix. In this case \n",
      "\n",
      "$$\n",
      "\\boldsymbol{S} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}\n",
      "$$\n",
      "\n",
      "In terms of the notation used for the rotation of matrices, $\\boldsymbol{R} = \\boldsymbol{U}^{T}$, where in this case $\\boldsymbol{R}$ is used to change the basis to one aligned with the eigenvectors:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{S} = \\boldsymbol{R}^{T} \\boldsymbol{\\Lambda} \\boldsymbol{R}\n",
      "$$\n",
      "\n",
      "Since $\\boldsymbol{R}$ is an orthogonal matrix,\n",
      "\n",
      "$$\n",
      "\\boldsymbol{\\Lambda} = \\boldsymbol{R} \\boldsymbol{S} \\boldsymbol{R}^{T}\n",
      "$$\n",
      "\n",
      "We can test this for a given matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a symmetric matrix\n",
      "S = A + A.T\n",
      "\n",
      "S= -S\n",
      "\n",
      "# Compute eigenvectors of S and print eigenvalues\n",
      "lmbda, U = np.linalg.eig(S)\n",
      "print(lmbda)\n",
      "\n",
      "# R matrix\n",
      "R = U.T\n",
      "\n",
      "# Diagonalise S\n",
      "Lambda = R.dot(S.dot(R.T))\n",
      "print(Lambda)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-3.26926434 -0.9944455  -0.19425012  0.11236256]\n",
        "[[ -3.26926434e+00   1.11022302e-16   2.77555756e-17   1.38777878e-16]\n",
        " [ -2.22044605e-16  -9.94445502e-01   6.93889390e-18   1.38777878e-16]\n",
        " [ -2.49800181e-16   0.00000000e+00  -1.94250119e-01   2.34187669e-17]\n",
        " [  2.22044605e-16   1.11022302e-16   4.33680869e-17   1.12362557e-01]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above matrix is diagonal, and the diagonal entries are the eigenvalues.\n",
      "\n",
      "\n",
      "## Power iteration\n",
      "\n",
      "It was shown the lectures that the maximum eigenvalue, and associated eigenvector, can be estimated via repeated matrix-vector products. The method is known as power iterations. Below is sample code for the power iteration."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create starting vector\n",
      "x0 = np.random.rand(S.shape[0])\n",
      "\n",
      "# Perform power iteration\n",
      "for i in range(10):\n",
      "    x0 = S.dot(x0)\n",
      "    x0 = x0/np.linalg.norm(x0)\n",
      "x1 = S.dot(x0)\n",
      "\n",
      "# Get maxiumum exact eigenvalue (absolute value)\n",
      "eval_max_index = abs(lmbda).argmax()\n",
      "max_eig = lmbda[eval_max_index]\n",
      "\n",
      "# Print estimated max eigenvalue and error \n",
      "max_eig_est = np.sign(x1.dot(x0))*np.linalg.norm(x1)/np.linalg.norm(x0)\n",
      "print(\"Estimate of largest eigenvalue: {}\".format(max_eig_est))\n",
      "print(\"Error: {}\".format(abs(max_eig - max_eig_est)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Estimate of largest eigenvalue: -3.26926434316\n",
        "Error: 3.1603608619e-11\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After just 10 iterations, the estimated eigenvalue is very accurate.\n",
      "\n",
      "We now demonstrate a case where the power iteration fails - namely, when the starting vector is orthogonal to the eigenvector associated with the largest eigenvalue. We compute a starting vector that is orthogonal to the eigenvector associated with the largest eigenvector, and then perform power iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create starting vector\n",
      "x0 = np.random.rand(S.shape[0])\n",
      "\n",
      "# Get eigenvector associated with maxiumum eigenvalue\n",
      "eval_max_index = abs(lmbda).argmax()\n",
      "evec_max = U[:,eval_max_index]\n",
      "\n",
      "# Make starting vector orthogonal to eigenvector associated with maximum \n",
      "x0 = x0 - x0.dot(evec_max)*evec_max\n",
      "\n",
      "# Perform power iteration\n",
      "for i in range(10):\n",
      "    x0 = S.dot(x0)\n",
      "    x0 = x0/np.linalg.norm(x0)\n",
      "x1 = S.dot(x0)\n",
      "\n",
      "# Print estimated max eigenvalue and error\n",
      "max_eig_est = np.sign(x1.dot(x0))*np.linalg.norm(x1)/np.linalg.norm(x0)\n",
      "print(\"Estimate of largest eigenvalue: {}\".format(max_eig_est))\n",
      "print(\"Error: {}\".format(abs(max_eig - max_eig_est)))   \n",
      "\n",
      "# Get second largest eigenvalue\n",
      "print(\"Second largest eigenvalue (exact): {}\".format(lmbda[np.argsort(abs(lmbda))[-2]]))  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Estimate of largest eigenvalue: -0.994445502319\n",
        "Error: 2.27481884087\n",
        "Second largest eigenvalue (exact): -0.994445502319\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is clear that in this case we have approached the second largest eigenvalue."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}